services:
  chainlit:
    platform: "linux/amd64"
    build:
      context: "."
      dockerfile: "Dockerfile"
    environment:
      - LLM_INFERENCE_PROVIDER=ollama
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_LLM_MODEL=mistral-nemo:12b-instruct-2407-q4_0
      - GOOGLE_GEMINI_LLM_MODEL=gemini-2.5-flash
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - HUGGINGFACEHUB_API_TOKEN=${HUGGINGFACEHUB_API_TOKEN}
    volumes:
      - .chainlit/.files:/code/.chainlit/.files
    ports:
      - "8000:8000"
    restart: always
    networks:
      - app
    depends_on:
      - ollama
      - ollama-setup

  ollama:
    image: ollama/ollama:${OLLAMA_TAG:-latest}
    platform: "linux/amd64"
    environment:
      - OLLAMA_KEEP_ALIVE="60m"
    volumes:
      - ollama:/root/.ollama
    ports:
      - "11434:11434"
    networks:
      - app
    restart: always
    # Enable GPU support using host machine
    # https://docs.docker.com/compose/how-tos/gpu-support/
    #deploy:
    #  resources:
    #    reservations:
    #      devices:
    #        - driver: nvidia
    #          count: all
    #          capabilities: [ gpu ]

  ollama-setup:
    image: curlimages/curl:${CURLIMAGES_TAG:-latest}
    command:
      - -X
      - POST
      - http://ollama:11434/api/pull
      - -d
      - '{"name": "mistral-nemo:12b-instruct-2407-q4_0"}'
      - -H
      - "Content-Type: application/json"
    networks:
      - app
    depends_on:
      - ollama

volumes:
  chainlit:
  ollama:

networks:
  app:
